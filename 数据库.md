# 基础知识

SQL和NoSQL的区别：

* SQL是关系型数据库，用二维表格组织数据，有哪些字段、字段类型是固定的；NoSQL是非关系型数据库，可以不用行和列，而用更合适的格式组织数据，比如键值对（Redis）、文档（MongoDB）
* SQL提供ACID保证的事务；NoSQL一般只提供阉割版事务，这也是它快的原因
* SQL可以垂直（单机）扩展；NoSQL基于CAP、BASE理论，可以水平（分布式）扩展

常见的SQL数据库：MySQL、PostgreSQL、Oracle Database、Microsoft SQL Server

常见的NoSQL数据库：Redis、MongoDB、Elasticsearch

CAP定理：一个分布式系统最多只能同时满足CAP三项中的两项

* 一致性（Consistence）：每个请求都返回最新数据或者错误
* 可用性（Availability）：每个请求都有无错误的响应，但不保证是最新的数据
* 分区容错性（Partition tolerance）：节点之间的任何信息延迟或丢失都不会停止整个系统运行

为什么不能同时满足三项：

* 因为网络故障不可避免，所以分区容错性必须被满足，否则只能部署单节点，这就违背了分布式的初衷
* 如果要满足一致性，只能等待数据同步完后才能服务，但网络分区可能导致数据同步时间无限延长，就会导致超时和出错的响应，不能满足可用性
* 如果要满足可用性，节点可以只用本地的数据提供服务，但不能保证数据是最新的，不能满足一致性

BASE理论：即使无法做到强一致性，但每个应用可以根据自身的业务特点，保证弱一致性

* 基本可用（Basically Available）：出现故障的时候，允许损失部分可用性
* 软状态（Soft state）：允许系统存在中间状态，而该中间状态不会影响系统整体可用性
* 最终一致性（Eventually consistent）：系统中的所有数据副本经过一定时间后，最终能够达到一致的状态

# MySQL

## 引擎

MySQL支持的数据库引擎有InnoDB、MyISAM，它们的区别：

* InnoDB支持事务，MyISAM不支持事务
* InnoDB支持外键，MyISAM不支持外键
* InnoDB使用聚簇索引，数据在文件中存储的顺序就是聚簇索引（主键）的顺序，即使没指定主键也会自动生成隐式的主键；MyISAM不支持聚簇索引
* InnoDB不存储表的行数，`COUNT(*)`需要全表扫描；MyISAM会存储表的行数
* InnoDB最小粒度的锁是行锁，MyISAM最小粒度的锁是表锁
* InnoDB使用日志保证可靠性；MyISAM不保证可靠性，崩溃后无法安全恢复

MySQL 5.5之后默认的数据库引擎是InnoDB，以下的讨论都是基于InnoDB的

## 索引

[参考](https://xiaolincoding.com/mysql/index/index_interview.html)

索引是帮助快速查找数据的数据结构，建索引是为了用空间换时间。索引不能建太多，否则会占用大量空间，而且写数据会变慢，因为还要维护索引

### 数据结构

MySQL的索引数据结构是B+树。B+树是多叉树，只有叶子节点存储数据，非叶子节点只存储索引不存数据，叶子节点之间用双向链表连接

为什么使用B+树：

* 相对于二叉树：多叉树的高度更小，读取每层需要一次磁盘IO，用多叉树就减少了大量IO次数。MySQL的B+树一般3层就可以存储上千万数据
* 相对于B树：
    * B+树的非叶子节点不存储数据，可以容纳更多索引，高度比B树更小
    * B+树的叶子节点之间用链表连接，范围查找的时候遍历更方便，而B树的遍历会涉及不同层级，增加IO次数
    * B+树插入、删除时树的变形更少，效率更高
* 相对于哈希表：
    * B+树的结点是有序的，范围查找的时候只需要从边界遍历，而哈希表是无序的
    * 哈希表在冲突较多的时候需要重哈希，会移动所有结点

### 分类

按物理存储分：

* 聚簇索引（主键索引）：叶子节点存储的是实际数据。如果指定了主键则主键就是聚簇索引，否则如果有不含NULL的唯一键则把它作为聚簇索引，如果都没有则隐式生成一个自增的的ID列作为聚簇索引
* 二级索引（辅助索引）：叶子节点存储主键。如果需要其他数据则需要回表，即根据主键再次查找聚簇索引

按字段特性分：

* 主键索引：必须唯一，且不能有NULL。主键索引也是聚簇索引
* 唯一索引：必须唯一，但允许有NULL
* 普通索引：不要求唯一，允许有NULL
* 前缀索引：指针对字符串的前几个字符建立索引，而不是用整个字段

### 优化

* 覆盖索引：如果要查询的所有字段在二级索引内全都包含，则不需要回表。比如二级索引是`(name)`，那么`SELECT id FROM ... WHERE name = ...`不会回表
* 索引下推：MySQL 5.6引入，对于联合索引，不符合最左匹配原则的条件也可以由引擎层判断而过滤，不用回表再交给server层判断条件。比如二级索引是`(a, b)`，执行`SELECT * FROM ... WHERE a > 1 AND b = 2`，查找索引时只能用`a > 1`的条件，但是`b`的数据也在索引里，引擎层可以判断`b`是否满足条件，而不用回表
* 提高区分度：建立联合索引时，把区分度大的字段排在前面，这样可以尽快排除大量数据。比如性别字段不适合排在靠前的位置，而UUID这样的字段适合排在靠前的位置
* 主键最好是自增的：使用自增主键，每次都是插入在最后一个节点的末尾，不需要移动数据，也不会导致页分裂
* 索引列最好是NOT NULL的：存在NULL值使优化器做选择时更加复杂，难以优化。因为NULL会使索引统计、值比较都更复杂
* 防止索引失效

索引失效的情况：

* 联合索引不符合最左匹配原则，或者字符串在开头使用了模糊匹配`LIKE '%...'`。因为联合索引和字符串都是从左到右比较的，比如建立`(a, b, c)`索引时，先使`a`有序，在`a`相同的时候再使`b`有序，在`a`、`b`相同的时候再使`c`有序。如果不符合最左匹配原则，就不能通过索引排除掉某些范围的数据，而只能全表扫描
* 在查询条件中对索引列做了函数计算、运算符计算、类型转换等操作。因为索引里存的是索引本身的值，而不是经过计算后的值。容易犯错的是隐式类型转换，MySQL在遇到字符串和数字比较时会把字符串转成数字。比如字符串列和整数比较`WHERE phone = 1300000001`会变成`WHERE CAST(phone AS signed int) = 1300000001`而使索引失效；但整数列和字符串比较则不会使索引失效
* `OR`条件中使用了没有索引的列
* 使用了否定条件，比如`!=`、`NOT IN`、`NOT EXISTS`

总之不能通过索引排除掉某些范围的数据，就会使索引失效

## 事务

### ACID

ACID是指事务的特性：

* 原子性（Atomicity）：一个事务中的操作，要么全部完成，要么全部不完成，不会结束在一个中间状态。如果事务在执行过程中出错，则会回滚到开始前的状态，就像这个事务从来没有执行过一样
* 一致性（Consistency）：事务操作前后，数据满足完整性约束。比如唯一键约束、外键约束
* 隔离性（Isolation）：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致
* 持久性（Durability）：事务一旦提交成功，对数据的修改就是永久的，即使系统故障也不会丢失

怎么保证ACID：

* 持久性通过redo log（重做日志）保证。提交事务在写入磁盘前先写日志，万一系统崩溃了也可以通过日志来恢复到事务完成的状态
* 原子性通过undo log（回滚日志）保证。修改数据之前先记录上一个版本的状态，需要回滚时就根据日志恢复到事务开始之前的状态
* 隔离性通过锁和MVCC（多版本并发控制）保证
* 一致性通过持久性 + 原子性 + 隔离性来保证

### 隔离级别

并发可能遇到的问题：

* 脏读：读到其他事务未提交的修改
* 不可重复读：在一个事务内多次读取同一个数据，前后两次读到的数据不一样
* 幻读：幻读是针对范围查询的，在一个事务内多次查询同一个条件的数据，前后两次读到的结果集数量不一样

事务的隔离级别：

* 读未提交（Read Uncommitted）：一个事务还没提交时，它的变更就能被其他事务看到
* 读已提交（Read Committed）：一个事务提交之后，它的变更才能被其他事务看到。可以防止脏读
* 可重复读（Repeatable Read）：一个事务执行过程中看到的数据，和它启动时看到的数据是一致的。这是MySQL默认的隔离级别。可以防止脏读、不可重复读。在MySQL中还可以防止大部分幻读问题
* 串行化（Serializable）：只读事务和写事务串行执行。可以防止脏读、不可重复读、幻读

MySQL如何实现这些隔离级别：

[参考](https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html)

* 读未提交：不使用锁和MVCC，直接读最新版数据
* 串行化：开启隐式事务的情况下把所有`SELECT`转换成`SELECT ... FOR SHARE`，也就是所有事务都用当前读，会加读写锁
* 可重复读：对快照读使用MVCC，在事务启动时创建Read View。对当前读使用记录锁、间隙锁、临键锁
* 读已提交：对快照读使用MVCC，在每条语句执行时创建Read View。对当前读使用记录锁，而间隙锁只用于检查外键、唯一键时

MySQL在可重复读级别怎么防止大部分幻读问题：

* 对于快照读，通过MVCC解决。如果有其他事务插入、删除数据，它的修改对当前事务是不可见的
* 对于当前读，通过间隙锁、临键锁解决。如果其他事务要插入数据，要先获得插入意图锁，但插入意图锁和间隙锁冲突，这个插入语句会被阻塞。同理如果要删除则要先获得记录锁

可重复读级别不能防止的幻读问题：当一个事务中混合了快照读和当前读就可能发生幻读。比如先`SELECT * FROM ... WHERE id > 100`，另一个事务插入了id = 200的记录并提交，然后本事务再`SELECT * FROM ... WHERE id > 100 FOR UPDATE`，这时就会多出前面插入的数据

### MVCC

MVCC是多版本并发控制，是用来提高并发性能的，它使只读的事务不会和写事务互相阻塞。MVCC是用于快照读的

* 快照读：不使用`FOR SHARE`、`FOR UPDATE`的`SELECT`语句。可能读到旧版的数据，即快照
* 当前读：使用`FOR SHARE`、`FOR UPDATE`的`SELECT`语句，以及`UPDATE`、`DELETE`这种修改数据的语句。读到的一定是最新数据，MySQL会加锁保证这一点

#### MVCC的实现

[参考](https://xiaolincoding.com/mysql/transaction/mvcc.html#read-view-%E5%9C%A8-mvcc-%E9%87%8C%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84)

Read View表示数据的一个快照版本，里面有4个重要字段：

* `m_ids`：创建Read View时还未提交的事务ID列表
* `min_trx_id`：`m_ids`中的最小值
* `max_trx_id`：创建Read View时数据库应该给下一个事务生成的ID，即`目前全局最大的事务ID + 1`
* `creator_trx_id`：创建该Read View的事务ID

通过这几个字段就可以判断某个事务ID做的变更在这个Read View是否可见：

* 如果事务ID < `min_trx_id`，说明创建Read View时这个事务已提交，可见
* 如果事务ID >= `max_trx_id`，说明创建Read View时这个事务还没有创建，不可见
* 如果事务ID在`[min_trx_id, max_trx_id)`范围内，则还需要判断是否在`m_ids`中
    * 如果事务ID在`m_ids`中，且不是`creator_trx_id`，说明创建Read View时这个事务还没有提交，不可见
    * 如果事务ID不在`m_ids`中，说明创建Read View时这个事务已提交，可见

另外聚簇索引中每个记录行有2个隐藏列：

* `trx_id`：写入这个记录的事务ID
* `roll_pointer`：指向undo log中上一个版本的记录，上一个版本的记录中的`roll_pointer`指向上上个版本，这样形成一个版本链

对每一行进行快照读时，先判断`trx_id`对该Read View是否可见，如果不可见再判断上个版本是否可见，一直寻找到某个可见版本，或者跳过该记录行

### 锁

参考：

* [MySQL有哪些锁](https://xiaolincoding.com/mysql/lock/mysql_lock.html)
* [MySQL是怎么加锁的](https://xiaolincoding.com/mysql/lock/how_to_lock.html)
* [官方文档](https://dev.mysql.com/doc/refman/8.0/en/innodb-locking.html)

#### 分类

* 表级锁
    * 表锁：共享表锁`LOCK TABLES ... READ`，独占表锁`LOCK TABLES ... WRITE`。表锁和行级锁会冲突，一般不使用，因为粒度太大了
    * 意向锁（Intention Lock）：共享意向锁指定事务想要在某些行上加共享锁，独占意向锁指定事务想要在某些行上加独占锁。意向锁之间不会冲突，和行级锁也不会冲突。意向锁是为了方便查找表里是否有记录被加锁，否则只能遍历锁来查找
    * 元数据锁（Metadata Lock）：进行增删改查操作时自动加共享MDL锁，对表结构进行修改时自动加独占MDL锁
    * AUTO-INC锁：插入记录用到自增变量（AUTO_INCREMENT）时加锁
* 行级锁
    * 记录锁（Record Lock）：锁定一条记录（索引）
    * 间隙锁（Gap Lock）：锁定记录（索引）之间的间隙，是一个开区间。间隙锁虽然分共享锁和独占锁，但是它们功能相同且不会冲突，因为它只是用来防止幻读的
    * 临键锁（Next-Key Lock）：等于间隙锁 + 记录锁，是一个左开右闭区间。因为间隙锁 + 记录锁的情况太常见了，遇到这种情况就会把两个锁合并成一个临键锁
    * 插入意向锁（Insert Intention Lock）：一种特殊的间隙锁，插入记录之前要先加插入意向锁。插入意向锁会和间隙锁、临键锁冲突。它是用来防止幻读的

#### 加锁方法

情况太多了，具体的看参考链接，这里只总结一些规则

* 锁是加在索引上的，如果没有找到对应的索引就会变成间隙锁
* 如果搜索的是主键索引则在主键索引上加锁；如果搜索的是二级索引则在二级索引上加锁，并且在主键索引上加记录锁
* 锁的范围尽量小，但要覆盖搜索条件
* 相邻的间隙锁、记录锁会变成一个临键锁（或者说优先上临键锁，但是有时边界会退化成记录锁和间隙锁）
* 索引的头部和末尾是负无穷大和正无穷大伪记录，伪记录也可以加锁

MySQL是一边扫描索引一边加锁的，如果查询条件没用到索引，则需要扫描全表，这样会对每一条记录加上临键锁，相当于锁全表

#### 死锁

一种死锁的情况：

1. 假设存在id = 1和10的记录
2. 事务A执行`UPDATE ... WHERE id = 2`，更新了0行，获得了`(1, 10)`的独占间隙锁
3. 事务B执行`UPDATE ... WHERE id = 3`，更新了0行，获得了`(1, 10)`的独占间隙锁
4. 事务A想插入id = 2的记录，需要先加`(1, 10)`的插入意向锁，但是这时事务B持有间隙锁，所以阻塞
5. 事务B想插入id = 3的记录，需要先加`(1, 10)`的插入意向锁，但是这时事务A持有间隙锁，所以阻塞

如何避免死锁：

* 设置事务等待锁的超时时间，当事务获取锁超时时，会回滚
* 开启主动死锁检测，检测到死锁时会回滚其中一个事务
* 将事务隔离级别设置为读已提交，这样不会加间隙锁，但是会有不可重复读、幻读问题

## 优化数据库

当数据量太大了，就会出现查询慢的问题。这时可以采用分库分表、读写分离等方式减少数据库负载

### 分库分表

分库是为了解决并发量太大的问题，因为单机的磁盘带宽、网络带宽、连接数、内存缓存都是有限的，分成多个结点的库可以减少每个结点的负载

分表是为了解决数据量太大的问题，数据量越大磁盘IO次数就越多。分表方法有垂直分表、水平分表：

* 垂直分表是按照业务、冷热数据等属性把不同字段拆分到不同的表。拆分后每行数据大小变小了，表中的页数会减少，磁盘IO次数也会减少
* 水平分表是按照主键的哈希把记录拆分到不同的表。拆分后B+树的高度变小，磁盘IO次数也会减少

缺点：事务、范围查询、数据聚合、join查询比较麻烦，需要应用层做特殊处理

适合分表的情况：数据之间没有关系，基本是按主键查找的；不需要复杂的事务、join查询

### 读写分离

使用主从复制把主库的数据同步到从库，然后主库负责写的事务，从库负责只读的事务。读写分离也是为了提高并发性能的，使只读事务和写的事务互不影响。读写分离适用于读多写少的情况

缺点：数据不一致，主库数据同步到从库需要时间，如果要求强一致性则不能用

# Redis

## 特点

Redis是基于内存的键值对数据库。它非常快，单机的QPS是MySQL的10倍，能轻松破10w，因此可以用作数据库缓存，提高查询性能

Redis为什么快：

* 大部分操作都在内存中完成，并且采用了高效的数据结构
* 单线程避免了频繁的上下文切换，而且也不用加锁
* 使用IO多路复用、epoll，一个线程就能处理多个客户端

### 线程模型

Redis单线程指的是读写数据用的是一个线程，而不是整个进程只有一个线程。除了主线程，Redis的线程还有：

* bio_close_file：用于关闭文件（`close`）
* bio_aof_fsync：用于AOF日志刷盘（`fsync`）
* bio_lazy_free：用于释放内存（`free`）
* Redis 6.0引入的网络IO线程（主要是`write`，也可以配置`read`）

Redis为什么使用单线程：

* 性能瓶颈不是CPU，更多是受内存大小和网络IO限制
* 多线程会增加系统复杂度，存在上下文切换、加锁造成的性能损耗

Redis 6.0为什么引入多线程网络IO：Redis的性能瓶颈通常在网络IO上，单线程无法充分利用网卡的性能，引入多线程是为了提高网络IO的并行度

## 数据类型

常用的数据类型：

* String（字符串）：可以表示任意二进制数据。数据结构是SDS（简单动态字符串）
* List（列表）：字符串的列表，可以在头尾快速插入、删除。数据结构是quicklist
* Hash（哈希表）：键值对的集合，可以表示一个对象。数据结构是listpack（元素少的时候）或哈希表
* Set（集合）：无序、唯一的字符串集合，可以做交、并、差运算。数据结构是整数集合（元素少的时候）或哈希表
* Zset（有序集合）：有序、唯一的字符串集合，和Set相比多了用来排序的score属性。数据结构是listpack（元素少的时候）或跳表

### 数据结构

#### listpack

listpack是内存紧凑的链表，每个结点存储在连续的内存上，结点存储了自己的大小，用来找到下一个结点的偏移量

listpack取代了以前的ziplist，因为ziplist有连锁更新的问题，极端情况下一个节点的大小变了，会导致后面所有结点的大小全部改变。listpack不存储上一个结点的大小，而且把自己结点的大小存储在结点末尾。正向遍历时用encoding和data计算大小，反向遍历时尝试读取上一个结点的大小并判断是否合法

#### quicklist

quicklist是多个listpack组成的双向链表。quicklist在插入时不用移动后面所有结点，而是只移动一个listpack里的结点；如果listpack的空间不够才创建一个新的listpack

#### 跳表

跳表是在双向链表的基础上加了索引层。索引层有指向下一层的指针。索引层不是每个结点都存，而是隔几个结点存一个。类似二分查找，查找结点时先从最高层遍历，如果下一个结点的值大于要查找的值则向下一层继续遍历，所以跳表查找的效率比较高，复杂度是O(logN)

跳表创建新结点的时候有概率生成上一层的结点，生成上一层的结点又有概率生成上上层的结点。在Redis中这个概率是25%，且层高有最大限制，在Redis 7.0中层高限制是32

为什么用跳表而不是树：

* 跳表的内存占用更灵活，这取决于概率参数的大小，它能比B树占用更少的内存
* 范围查找更简单，不用涉及不同层次，缓存局部性至少和平衡树一样好
* 实现更简单，不用调整树的结构

## 缓存

### 过期策略

Redis的过期策略是惰性删除 + 定期删除

惰性删除是不主动删除键，而是访问的时候判断是否过期，如果过期则删除

* 优点：只有访问时才判断是否过期，节省CPU
* 缺点：已经过期的键一直没被访问，就不会被删除，浪费内存

定期删除是每隔一段时间随机抽取一定数量的键判断是否过期，如果过期则删除

* 优点：能删除没被访问的键，节省空间
* 缺点：不好确定执行的时长和频率，如果执行太频繁则会浪费CPU，如果执行太少则不能及时删除

Redis的定期删除：默认每秒进行10次检查，每轮从过期字典抽20个键判断过期并删除；如果本轮过期的键不超过5个，或者整个循环时间超过了25ms，则结束，否则继续下一轮

### 淘汰策略

[参考](https://redis.io/docs/reference/eviction/)

如果Redis占用的内存超过了配置的最大内存，则会淘汰一些键，直到内存占用小于阈值。淘汰策略有：

* noeviction：不进行淘汰，内存满后不接受写入，只能查询。这是默认的策略
* 只淘汰设置了过期时间的键：
    * volatile-random：随机淘汰
    * volatile-ttl：优先淘汰过期时间最早的
    * volatile-lru：优先淘汰最久未使用的
    * volatile-lfu：优先淘汰使用频率最小的
* 淘汰所有键：
    * allkeys-random：随机淘汰
    * allkeys-lru：优先淘汰最久未使用的
    * allkeys-lfu：优先淘汰使用频率最小的

Redis的近似LRU算法：每次随机抽5个键，淘汰最久未使用的那个

近似LRU的优点：

* 不用维护一个大链表，节省空间
* 不用每次访问都移动链表结点，节省CPU

Redis的近似LFU算法：记录键的上次访问时间和逻辑频率，每次访问键的时候，根据上次访问时间对逻辑频率做衰减，然后按照一定概率对逻辑频率 + 1。淘汰时同样也是随机抽取并淘汰逻辑频率最小的

### 缓存一致性

当Redis作为数据库缓存时，就会有数据库和缓存不一致的问题。无法做到强一致性，因为引入缓存就是用一致性换可用性，但是可以让不一致的时间尽量短，做到最终一致性

Redis作为缓存一般用旁路缓存模式（Cache-Aside）：读取时先从缓存读，如果缓存没有则从数据库读，然后写入到缓存

写入时一般有以下的策略：

* 先更新缓存，再更新数据库
* 先更新数据库，再更新缓存
* 先删除缓存，再更新数据库
* 先更新数据库，再删除缓存
* 延迟双删：在更新数据库和删除缓存的基础上，延迟一段时间，再次删除缓存

其中双更新、先删除缓存再更新数据库，在并发时都容易出现数据不一致的问题。双更新还有一个缺点就是缓存不一定被用到，这时就白更新缓存了

而先更新数据库，再删除缓存虽然也有不一致的问题，但是概率很小。因为不一致要满足3个条件：1. 缓存已失效；2. 读写并发；3. 更新数据库 + 删除缓存的时间 < 读数据库 + 写缓存的时间。但是删除缓存失败的时候还是会有不一致的问题，这时就要引入消息队列来重试。所以对一致性要求不高的情况可以用

延迟双删中的延迟执行一般用消息队列来做，如果失败了自然可以用消息队列来重试，不会增加很多复杂度。延迟的时间要比读数据库 + 写缓存的时间稍大

还有一种方法是用Canal中间件订阅MySQL的binlog，然后投递给消息队列再删除缓存，这样写数据库的线程就不用对缓存做额外处理了

### 缓存雪崩/击穿/穿透

缓存雪崩：大量缓存在同一时间过期，导致请求全都要查询数据库，使数据库的压力骤增。解决方法：

* 设置过期时间时加一个随机数，防止同时过期
* 用后台线程定期更新缓存，使缓存不会过期
* 限制同时查询数据库的线程数，比如加分布式锁

缓存击穿：某个热点数据过期了，导致查询它的请求要查询数据库，使数据库的压力骤增。解决方法：

* 用后台线程定期更新缓存，使缓存不会过期
* 限制同时查询数据库的线程数，比如加分布式锁

缓存穿透：要查询的数据既不在缓存，也不在数据库，导致无法构建缓存，每次请求都要查询数据库。解决方法：

* 数据库也没有数据时，可以缓存空值或默认值
* 使用布隆过滤器快速判断数据库中是否存在数据。布隆过滤器只能判断可能存在，不是一定存在，但布隆过滤器判断不存在，就一定不存在

## 事务

[参考](https://redis.io/docs/manual/transactions/)

Redis的事务不提供ACID保证，可能部分失败，执行后也不能回滚。它只提供以下的保证：

* 事务中所有命令按顺序执行，执行事务中的命令时不会处理其他客户端的请求，这保证事务作为单个独立操作执行
* 只有收到`EXEC`命令才会执行事务中的命令，如果客户端断线，则不会执行
* 另外事务还支持CAS，先用`WATCH`监视某些键，如果`EXEC`时被监视的键改变了，则事务会失败

使用方法：

1. （可选）使用`WATCH`监视某些键
2. 使用`MULTI`命令开启事务
3. 发送其他命令，这时命令不会被执行，只是进入队列
4. 使用`EXEC`执行事务。注意，即使任意命令失败也不会停止执行，后面的命令也会执行

为什么Redis不支持事务原子性和回滚：

* Redis命令通常不会失败，失败可能是语法错误，或者类型错误，这些应该在开发时就发现了
* 支持回滚会对Redis的简单性和性能产生重大影响

事务和流水线（pipelining）的区别：流水线是客户端提供的减少RTT的技术，它一次发送多个请求，然后读取多个响应，而不用每次发送都等待响应。流水线并不保证一次发送的请求会被服务器连续处理，中间可能插入其他客户端的请求

## 持久化

[参考](https://redis.io/docs/management/persistence/)

### RDB

RDB（Redis Database）是某个时间点的全量数据快照，RDB的格式很紧凑，适合用于备份

优点：

* 格式紧凑，节省空间
* 重启时恢复更快

缺点：

* 因为是全量数据，写入开销大，不能频繁持久化，宕机时会丢失更多数据
* 如果使用的内存太大，`fork`时复制页表会消耗更多时间，会阻塞请求

使用方法：

* 执行`SAVE`命令会在主线程生成RDB文件，会阻塞主线程
* 执行`BGSAVE`命令会`fork`一个子进程来生成文件
* 可以配置自动执行`BGSAVE`，每N秒检查如果至少有M个修改，则执行

### AOF

AOF（Append Only File）是每个写请求的日志，重启时重放这些日志就能恢复数据

优点：

* 每次只写入这次请求的日志，而不是全量数据，开销小
* 宕机时丢失的数据很少

缺点：

* 占用空间通常比RDB大
* 重启时恢复更慢

执行命令后只是用`write`写到内核缓冲区，不一定写到磁盘上了，但可以配置什么时候刷盘，有以下策略：

* No：不主动刷盘，由操作系统控制刷盘的时机。性能最好，操作系统宕机时可能丢失很多数据
* Everysec：每秒刷盘。性能中等，操作系统宕机时最多丢失1秒数据
* Always：每次写日志后都刷盘。性能最差，操作系统宕机时丢失的数据最少

AOF文件太大时会触发重写机制：

1. 主进程`fork`一个子进程，然后继续响应请求。重写过程中的请求除了写到旧的AOF文件，还要写到AOF重写缓冲区
2. 子进程把所有数据用一条命令记录到新的AOF文件。因为只用一条命令，所以文件大小会比旧的AOF文件小
3. 子进程重写完成后，给主进程发信号
4. 主进程处理信号，把AOF重写缓冲区里的日志也写到新的AOF文件，然后用新的AOF文件覆盖旧的AOF文件

### 混合式

混合式混合了RDB和AOF的优点。它文件前面记录的是RDB格式的数据，后面是AOF格式的数据

混合式持久化工作在AOF重写阶段，子进程重写文件时用RDB格式，而不是AOF格式

## 分布式扩展

### 主从复制

可以配置Redis服务器为其他节点的从节点。主节点可以接受读写请求，从节点只接受读请求。主节点会把写请求同步给从节点，保持数据一致。当主节点宕机，从节点可以继续提供服务

第一次同步过程：

1. 从节点连接主节点，然后发送`PSYNC ? -1`命令
2. 主节点回复`FULLRESYNC {Replication ID} {offset}`，表示使用全量同步，Replication ID是主节点本次启动时随机产生的ID，offset是命令偏移量
3. 主节点生成RDB文件，并把文件发送给从节点。这个过程中主节点还要把新的命令写到复制缓冲区
4. 从节点加载完RDB后，回复确认给主节点
5. 主节点再把复制缓冲区的命令发给从节点

后续命令同步也是通过这个连接同步。同步是异步的，主节点不会等到从节点同步成功才返回响应，所以会有短期不一致

连接断开重连后的同步：

1. 从节点发送`PSYNC {Replication ID} {offset}`，表示从这个命令偏移量开始同步
2. 因为复制缓冲区是环形缓冲区，如果太久没同步，旧的命令会被覆盖。主节点要判断从节点请求的命令偏移量是否被覆盖：
    * 如果没被覆盖，直接增量同步，把新的命令发给从节点
    * 如果被覆盖，只能全量同步，过程和第一次同步一样

为了避免断线重连时频繁地全量同步，可以把复制缓冲区的大小调大

为了保证数据一致，主从复制时，从节点不会删除过期的键，也不会在内存满时淘汰。而是主节点删除键时，生成一条`DEL`命令给从节点

如何防止延迟、脑裂产生的数据丢失：

* 配置min-slaves-max-lag，指定主从同步的延迟不能超过一定时间，否则禁止写数据
* 配置min-slaves-to-write，指定主节点至少要有几个从节点连接，否则禁止写数据

### 哨兵模式

哨兵模式是用来自动进行故障转移的。哨兵节点会监控主节点是否存活，如果主节点宕机了，则把一个从节点切换为主节点，并通知其他从节点和客户端

判断主节点下线：

1. 哨兵节点定时PING主节点
2. 如果主节点没在配置的时间内回复正常响应，则认为它主观下线
3. 当一个哨兵节点判断主节点主观下线后，会询问其他哨兵节点主节点是否下线
4. 如果认为主节点下线的哨兵节点数达到quorum配置的值，则认为主节点客观下线

选出负责故障转移的哨兵节点：

1. 认为主节点客观下线的哨兵节点成为候选者，候选者发起投票选出Leader
2. 每个哨兵节点只有一次投票机会，只有候选者能投给自己
3. 拿到半数以上投票，且票数 >= quorum的节点成为Leader。如果没有满足条件，则重新选举

故障转移：

1. 在从节点里选一个切换为新主节点
2. 把其他从节点的复制上游改成新主节点
3. 把新主节点的信息通过发布/订阅机制通知给客户端
4. 继续监控旧主节点，当它上线时把它设置为新主节点的从节点

选择新主节点：

1. 把网络不好的节点去掉，和主节点断开时间太长则认为网络不好
2. 判断节点配置的优先级，优先级高的优先
3. 如果优先级相同，判断复制进度，复制进度大的优先
4. 如果都相同，运行ID小的优先

### 集群模式

[参考](https://redis.io/docs/reference/cluster-spec/)

集群模式和主从复制不一样，不是每个节点都有所有的数据，而是每个节点有部分切片的数据。所有的数据分成16384（16k）个哈希槽，每个节点负责服务一部分哈希槽里的数据

为什么用哈希槽而不是一致性哈希：一致性哈希在迁移时需要重新计算所有键的节点，而哈希槽迁移时只需要以槽为单位分配，不影响所有的键

为什么是16384个哈希槽：

* 心跳包里有本节点管理的哈希槽的信息，如果槽数太多，心跳包会很大。而16k个哈希槽用bitmap只占用2kB的空间
* 一个集群的主节点数不太可能超过1000个，16384个哈希槽够用了

如果客户端请求的键不在本节点负责的哈希槽内，服务器会回复MOVED错误，让客户端再向指定的节点发送请求

在哈希槽迁移的过程中，可能一部分键在源节点，一部分键在目标节点。如果客户端请求的键不在本节点，服务器会回复ASK错误，让客户端再向指定的节点发送请求。客户端必须先发送ASKING，再发送真正的请求，否则目标节点还是会回复MOVED错误

`MGET`等批量操作和事务所使用的键必须在同一个节点内。可以使用哈希标签保证多个键被分配到同一个哈希槽，哈希标签是键中出现在第一个花括号`{}`内的字符串，计算哈希槽时只使用哈希标签来计算。比如`{user1000}.following`的哈希标签是`user1000`
