自我介绍

死循环检测是怎么做的
    用了一个类似看门狗的机制，每个worker线程处理完消息之后，会向一个状态统计的模块那边更新一个时间戳。然后另外有一个监控的线程，它会定期去检测所有线程的时间戳。如果有一个时间戳很久没有被更新了，那么判断它可能是死循环了

你对全区全服的架构了解吗
    按我的理解的话它应该是一个区服所有的服务器模块都有，所有玩家都是分布在这个区服里面的
具体架构是怎样的有了解过吗
    介绍了项目架构
比如其中一台gameserver宕了，玩家整个链路的转变是怎么样的，是直接退出到登录界面重新发起登录流程吗
    是会重新登录，但是我们客户端表现不会退出到一个登录界面。我们客户端表现就是转菊花转一下，是客户端内部自己去做重连的
    重连的话就是向gateserver发一个登录的请求，然后gateserver会选择一台gameserver转发过去。因为原来的gameserver宕机了，这时候会选择一台新的gameserver发过去
这个选择的过程用了什么算法
    我们是用了一致性哈希的
可以大概说一下一致性哈希算法吗
    一致性哈希它的思想就是如果我增加或者删除一个节点，它尽量保证大部分数据都不要迁移，还是在原来的节点。经常用的一个实现就是哈希环，就是哈希出一个随机的整数，我们假设它是分布在一个环上面的，节点就是环上面的一个数嘛，哈希之后我会找到大于这个数的第一个节点，作为它的哈希结果
你知道这个架构最多能承载多少人吗
    全服的话我不记得了，我记得单个gameserver大概能承载多少

排行榜是做在哪个服务器上，具体业务是做什么的排行呢
    我们排行榜是做在单独的服务器上的。排行是玩家去打关卡，关卡会有一个积分，我们排的是这个关卡的积分
是实时地排吗
    是实时的
你们用到什么数据结构吗
    我们排行榜是分成两种，一种是全量的，就是只要有玩家上报上来就给他保存到里面，另一种是只保存前几百个的排行榜
    全量的我们是用Redis来实现的，用了Redis的Zset的数据类型。前几百的话我们是自己用map做在进程里面的
了解过Zset底层的实现吗，内部使用了什么数据结构，还有时间复杂度
    Zset底层是一个跳表嘛，跳表就是在原来双向链表的基础上加了很多层，高层的作为索引，高层的结点数会少一点
    查找的时候是类似于二分查找的算法，它的时间复杂度是O(logN)级别的
那全量的排行榜你们上线之后有监测过它的数据量有多大吗
    这个没有监控，但我自己看过
那你们有做过这方面的性能测试吗
    这个比较早了，我来的时候可能已经做过了
那如果让你来做，你要怎么做这个性能测试，怎么去评估它的性能
    比如我可以随机出一些分数然后把它插入到排行榜里面吧，然后去计算它的QPS啊，整个大概能容纳多少人这样吧
你会关注哪一些指标
    像QPS啊，还有服务器处理消息的队列有没有积压吧，处理消息的时间大概是多久
之前有做过这方面的工作吗
    压测我们都是交给具体某一个人的

我看到你用了boost的Asio，还有libco这样的协程库。协程你们是以什么单位为基准去分配一个协程的
    我们是有需要就去分配一个，比如某个玩家在处理消息的时候，可能我需要去另外的服务器拉一些数据，拉数据的过程就是开了一个协程的
比如你gameserver收到客户端的请求，需要去后台的服务去请求的话，这时候就会创一个协程，然后去做这个逻辑的处理是吧
    是的，就只有部分的消息需要协程
部分是指哪一些呢
    主要是一些你需要去其他服务器拉一些数据的

除了这些还做过哪些系统呢，挑几个做时间比较长的，相对复杂的业务模块就行
    主要是一些活动吧，活动就做一些增删改查，不过我们会有一些条件的校验什么的
    可能做过最麻烦的最久的，它是一个周报系统。就是每周要统计一下玩家上周取得了什么成就啊，什么排名啊，获得了什么物品之类的
    它主要是麻烦在，它的数据不止分布在一个服务器上的，而且可能每个系统的结算时期也是不一样的
那这些数据你是怎么汇集的，是汇集到哪一台服务器，怎么实现的
    最后还是让玩家自己看的，所以最后是汇总到玩家服务器上面
    我是这样实现的，玩家上线的时候，或者是临近结算的时候，会开一个定时器。然后在回调的时候，开启一个拉数据的协程，拉完数据就存储到玩家自己的数据里面
那玩家的数据是存在gameserver上面吗
    对，gameserver它本身会有玩家数据的缓存嘛
然后在gameserver上面做汇集，然后去写Redis和MySQL吗
    有一些是存在Redis吧，其他的跟着玩家数据一起存到MySQL了

你工作中有没有遇到一些棘手的问题
    线上的问题还好，因为我们的日志会做得比较详细，详细到你获取什么奖励啊，获取什么物品，或者消费什么物品，像是获取每个奖励的原因我们都会记录下来，所以日志会做得比较全面
    我们一般发现问题可能就是出了什么BUG，漏发一些奖励什么的。这时候我们会让数据平台那边去拉一下日志，它可能达成了什么条件，但是没领到奖励的。然后会用运营的手段去补偿，比如给他们补发一些邮件什么的

MySQL这边，以你做过的系统啊，表里面有什么东西，建索引之类的，可以拓展性地说一下吗
    我们MySQL表结构基本上是不变的，因为我们大部分数据就是玩家的数据嘛，玩家的数据就是序列化之后存到MySQL表的一个字段里面的
    我以玩家数据的表来举例的话，因为玩家数据会比较多，所以我们是把玩家数据给分库分表了。它是分成了10个库，每个库会有10张表。我们根据玩家UID的最后两位数，来哈希到某一个库的某一张表里面
    玩家数据的表结构主要就是UID、玩家昵称、玩家的等级和经验之类的字段，其他的它会有一个bin的字段，就是玩家序列化之后的数据
那这个bin的序列化的字段里面是包括各种背包信息、道具信息什么的，全都存到唯一的bin里面吗
    是的
那比如背包加一个道具，岂不是所有系统都要重新序列化，然后存到这个bin里面
    是这样的，所以我们是定期去保存的，比如2分钟或者5分钟保存一遍
那没有想过把这个bin按业务模块拆开吗
    我觉得这个怎么设计主要还是看业务的吧，因为我们主要还是看玩家的业务，它可能你作为一个整体去保存，可以保持它的一个完整性
    如果你拆开的话，比如背包里面要加一个东西，其他系统可能又要扣一个东西，如果分开保存的话可能不能保证原子性

你有用过哪一些IO多路复用的接口
    我说下我了解的，IO多路复用主要是有三种嘛，select、poll还有epoll
epoll讲一讲它的原理吧，包括水平触发、边缘触发那些，包括内核里面是怎么实现的，都用到什么数据结构
    epoll的话它内部是用了回调去实现的，就是第一次添加一个文件描述符时，epoll内核里面会给这个文件的驱动程序去添加一个回调，在回调里面当事件发生的时候，往我们的就绪队列里面添加一个事件
    文件描述符的数据结构是红黑树，就绪队列的数据结构是一个链表，所以这个会比较高效
    epoll它有个特点就是它把添加文件描述符，还有等待的接口给分开了，这个是克服了以前的像select还有poll需要频繁拷贝事件的一个问题。而且它拷贝回去都是已经就绪的事件，那么用户就不需要遍历一遍，去找出哪些事件是就绪的
    epoll是支持水平触发和边缘触发。水平触发就是只要这个文件描述符可以读，就会一直报告这个可读的事件。边缘触发就是只有第一个这个文件描述符可读的时候才报告可读的事件，如果你没有读完这个缓冲区，那么下一次就不会报告这个事件
那什么时候才会再次报告呢
    当你读完这个缓冲区，下一次再可读的时候
数据可读是协议栈那边一收到有内容添加到缓冲区里面就可读了吗
    应该是这样吧

protobuf有了解过它内部的原理吗
    protobuf主要就是一个用于序列化的库嘛，它是设计了proto的语言，用来定义数据结构。我们会定义一些字段类型、字段名称，还有tag什么的
    序列化之后它会写入tag、数据类型，还可能会写入长度，然后写入数据的值

你们网络库就是用的Asio吗
有深入地看过它的源码吗
    没深入看过，我了解它是Proactor的设计模式吧
可以把你知道的都说一说
    Proactor的话它是分为handler、proactor还有async processor嘛。用户在使用的时候是创建一个handler，创建一个回调吧，然后把异步操作和回调交给异步操作器去处理，操作完之后就会调用proactor，然后回调我们的handler
    Asio的话它是封装了一个协程，我们可以在协程里面发起一个异步操作，然后等数据读到我们的缓冲区之后，才唤醒我们的协程

模拟抽卡的概率，这个概率要保证和玩家抽到的概率一致，这个是怎么理解呢
    我们是说整体的概率吧，就是从统计上说的话，抽卡抽到无限次的话，它会有一个掉落的期望，会有一个统计出来平均的概率。我们是希望这个概率和我们对外声称的概率是一致的
那这个过程是怎么做的呢
    因为我们的抽卡机制会比较复杂，它不能够直接模拟算法啊，或者调用它的接口什么的。所以我们最终会从线上拷贝真实的玩家数据，拷贝到一个测试服。然后我们会用一个机器人程序，去登录这些玩家，然后模拟向服务器发送抽卡的请求
    抽卡之后，服务器会把抽卡结果写到日志里面。然后我们去分析这个日志，看它的掉落期望是不是和我们的预期相符
拷贝是怎么拷贝的，直接录制客户端的网络协议吗
    我们就是直接从数据库里面把它拷贝过去嘛，就是从线上数据库读取玩家数据，然后写入到测试服的数据库
那拷贝的就是他已经抽卡，得到最终的那个物品的信息是吗
    没有，他可以不抽卡
数据库那边是记录了他的行为，这次抽了什么东西吗
    没有这种记录

反问
    我了解到你们是一个TPS项目，像这种的话服务器这边会有什么难点吗
        我们做的也是全区全服的架构，然后主要是考虑到以后承载的人数多吧，我们是做了主备，还有灾备的架构，写起来会更复杂一些。这是局外的吧，然后局内的话，比如我们这种FPS，跟传统的还不一样，它的子弹，它的技能，它的CD都很短，所以在同一帧内可能会有大量的对象，这就导致我们的同步会更加复杂
    你们是帧同步还是状态同步
        结合的
    就服务器也会参与战斗逻辑之类的
    开发周期大概是怎么样的
        从立项到现在两年多了，计划是在明年小范围的上线吧。具体也是领导层决定的，所以我们这边也不太清楚
    那平时是多久一个版本呢
        说不准，今年到现在都四五个版本了，基本上是一个多月会有一个版本


------
感觉和之前面的很多重复啊
这次问到的架构承载量、框架源码方面不够熟悉
第二天反馈能力不错，但是有更便宜的人选了。唉，打工人就不要卷工资了吧
