# 协议栈

* OSI七层模型：应用层、表示层、会话层、传输层、网络层、数据链路层、物理层
* TCP/IP四层模型：应用层、传输层、网络层、网络接口层。其中应用层对应OSI的应用层、表示层、会话层；网络接口层对应OSI的数据链路层、物理层

OSI模型结构有些复杂、冗余，因此一般使用的是TCP/IP四层模型

1. 物理层：规定信号是怎么传输的，比如用双绞线、光纤、电磁波；怎么编码；信号传输频率

    * 传输的数据单位是位（bit）
    * 没有地址的概念，连在一条线上的设备都能收到信号，只是网卡会过滤掉不是发给自己的数据帧
    * 相关设备：网卡、集线器

2. 数据链路层：提供同一个网络内主机之间的通信服务

    * 传输的数据单位叫做数据帧（frame）
    * MTU（最大传输单元）：每个数据帧中最大负载大小（不包括数据帧头部），以太网一般是1500字节；任天堂Switch默认是1400；实际开发时为了保险可以用1300
    * 地址：MAC地址（硬件地址、物理地址）
    * 常见的协议：以太网、IEEE 802.11（WiFi）
    * 相关设备：网卡、交换机

3. 网络层：提供跨网络主机之间的通信服务，包括路由和寻址功能

    * 传输的数据单位叫做数据包（packet）或者分组
    * 地址：IP地址
    * 常见的协议：IPv4、IPv6、ICMP
    * 相关设备：路由器

4. 传输层：提供应用程序之间的通信服务

    * TCP一次传输的数据单位叫做数据段（segment），UDP的叫做数据报（datagram）
    * MSS（最大分段大小）：每个数据段中最大负载大小（等于 MTU - IP头大小 - TCP头大小），一般是1460字节
    * 地址：端口号
    * 常见的协议：TCP、UDP

5. 会话层：为终端用户应用程序之间提供创建、关闭、管理会话的机制

    * 功能：认证、权限、会话恢复
    * 常见的协议：SOCKS、RPC

6. 表示层：提供主机数据格式和网络标准数据格式之间的转化，功能包括序列化、加密、压缩等
7. 应用层：提供网络应用服务

    * 常见的协议有：DNS、HTTP、FTP、SMTP、SSH、Telnet

## 发送一个包的过程

1. 如果只知道域名，不知道IP，则先通过DNS获取IP
2. 通过IP和子网掩码判断是否和本机在同一个网络，如果在同一个网络则链路层直接发给目标MAC地址，否则发给网关（路由器）

    * 如果不知道IP对应的MAC地址，则先广播ARP请求获取MAC地址
    * 目标主机收到ARP请求，发现查询的是自己的IP，则发ARP响应，告知自己的MAC地址
    * 本机收到响应后将IP对应的MAC地址存到自己的缓存表

3. 路由器根据路由规则发给下一个节点，重复这个过程直到发到目标主机

## DNS

查询过程：

1. 先在本机hosts设置和DNS缓存里查找，如果找到了则返回
2. 发送请求给系统设置的本地DNS服务器，如果这个服务器有缓存则返回
3. 本地DNS服务器向根域名服务器请求顶级域名服务器地址
4. 本地DNS服务器向顶级域名服务器请求解析，如果顶级域名服务器能解析则返回，否则返回二级域名服务器的地址
5. 如果返回的是二级域名服务器地址，本机再向二级域名服务器请求解析……重复这个过程直到解析出目标域名的IP

实际上DNS会有很多缓存，几乎不会请求到根域名服务器和顶级域名服务器

### 递归查询与迭代查询

* 递归查询：用户只发出一次DNS请求，域名服务器代为向下一级域名服务器发出请求，最后给用户最终的IP
* 迭代查询：域名服务器可能返回下一级域名服务器的地址，由用户自己向下一级再次发送请求

一般用户到域名服务器的查询是递归查询，域名服务器之间是迭代查询

### DNS记录

一条记录包括：`(name, value, type, ttl)`

* TTL：这条记录能被其他DNS服务器缓存多久
* 常用的type：
    * A：IPv4地址
    * AAAA：IPv6地址
    * CNAME：表示name是别名，value是规范主机名，应该再去查规范主机名的IP地址作为别名的IP地址
    * MX：邮件交换记录，表示发往name域名的邮件应该发到value域名指向的服务器

## TCP

TCP是传输控制协议，是面向连接的、可靠的、基于字节流的传输层协议

* 应用：HTTP、FTP、SMTP、SSH。大部分应用的传输层都用TCP，因为不用考虑可靠性、乱序等问题

如何保证可靠性：

* 用SEQ（序列号）表示这个包的数据偏移量
* 用ACK（确认号）表示自己确认收到了这个SEQ之前的数据。其值等于最后一个连续收到的包的SEQ + 包的大小
* 收到数据后需要发送ACK包确认自己收到了数据。如果超过一定时间没有收到确认，则认为数据丢失，需要重传

如何保证收到数据顺序：

接收方根据SEQ对数据排序。如果SEQ < 自己当前的ACK，则是重复的数据，丢弃；否则根据SEQ塞到接收缓冲区相应的位置

TCP能否检测连接断开：

* 如果是进程崩溃，内核会负责发FIN包断开TCP连接
* 如果是网络因为物理原因断开（比如断电），则要看有没有要发送的数据
    * 如果有数据要发送，因为长时间没有收到确认，发送方会认为连接异常断开
    * 如果没有数据发送，看是否开启SO_KEEPALIVE选项，开启后会定时发送一个探测包，对端回复一个ACK，这样才能确认连接没有断开。但是按默认配置要超过2小时才能检测到，所以比较鸡肋，这也是应用层要发心跳包的原因
    * 如果没有开启保活，则要等到下次发送数据确认超时，或者收到了RST包，才能知道连接断开

### 状态

* LISTEN：服务器监听中，正在等待SYN包
* SYN-SENT：客户端已经发送SYN包，正在等待服务器响应SYN+ACK包
* SYN-RECEIVED：服务器收到了SYN包，已经发送SYN+ACK包，正在等待客户端响应ACK包
* ESTABLISHED：连接已经建立，可以发送数据了。对于客户端，在SYN-SENT状态收到SYN+ACK包后进入这个状态；对于服务器，在SYN-RECEIVED收到ACK包后进入这个状态。TCP连接建立后双方是对等的，没有客户端、服务器之分了
* FIN-WAIT-1：连接建立后，主动调用close的一端进入这个状态。已经向对端发送FIN包，正在等待对端的ACK包确认收到这个FIN包
* FIN-WAIT-2：在FIN-WAIT-1状态，收到了对端的ACK包。对端知道本端没有数据要发了，但是对端可能还有数据要发，此时正在等待对端的FIN包
* CLOSE-WAIT：连接建立后，收到了对端的FIN包。本端可能还有数据要发，正在等待本端调用close
* CLOSING：在FIN-WAIT-1状态，先收到了对端的FIN包而不是ACK包才会进入这个状态，概率很小。此时两端都没有数据要发了，但是本端还不知道对端是否收到自己的FIN包，正在等待对端的ACK包
* LAST-ACK：在CLOSE-WAIT状态，本端调用close后进入这个状态。此时本端发送了FIN包，但是不知道对端是否收到，所以要等待对端最后一个ACK包
* TIME-WAIT：在FIN-WAIT-1状态，接受到FIN包和ACK包后进入这个状态。此时本端发送了ACK包，但是不知道对端是否收到，所以要等待一段时间（2MSL），如果对端没有重传，说明对端已经收到ACK包并断开连接，本端才可以断开
* CLOSED：在LAST-ACK状态收到ACK包，或者在TIME-WAIT状态等待时间结束。此连接双方已经断开了

相关系统调用：

* 创建一个socket，刚创建时在CLOSED状态

```c++
int socket(int domain, int type, int protocol);

// AF_INET：使用IPv4协议
// SOCK_STREAM：流式传输
// IPPROTO_TCP：传输层用TCP协议
int fd = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);
```

* 服务器调用listen后进入LISTEN状态
* 服务器要绑定一个地址用来监听；客户端不需要调用，connect时会随机分配一个
* 绑定0.0.0.0地址表示监听所有网卡，而127.0.0.1地址表示只监听本地回环网卡，即只有本机的客户端能连接

```c++
int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);
int listen(int sockfd, int backlog);

sockaddr_in addr;
memset(&addr, 0, sizeof(addr));
addr.sin_family = AF_INET;
addr.sin_addr.s_addr = inet_addr("0.0.0.0");
addr.sin_port = htons(8080);
bind(listen_fd, &addr, sizeof(addr));
listen(listen_fd, 5);
```

* 客户端调用connect发送SYN包，经过SYN-SENT状态后变成ESTABLISHED状态，然后connect才返回

```c++
int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen);

int res = connect(fd, &server_addr, sizeof(server_addr));
```

* 服务器中SYN-RECEIVED状态的连接会进入SYN队列，第三次握手后变成ESTABLISHED状态，且连接从SYN队列转移到accept队列
* accept从accept队列中取出一个连接，此时已经是ESTABLISHED状态了
* 如果一直不调用accept，队列满了之后，服务器会忽略掉第三次握手的ACK包，使服务器端不会进入ESTABLISHED状态
* accept队列大小由listen的backlog参数指定，`accept队列大小 = min(系统somaxconn配置, backlog)`

```c++
int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);

int fd = accept(listen_fd, &client_addr, &client_addr_size);
int fd = accept(listen_fd, nullptr, nullptr);
```

* ESTABLISHED状态中可以调用recv、send等函数对缓冲区读写数据
* recv和read区别：recv只能用于socket fd，而且比read多一个flags参数

```c++
ssize_t recv(int sockfd, void *buf, size_t len, int flags);
ssize_t read(int fd, void *buf, size_t count);
ssize_t send(int sockfd, const void *buf, size_t len, int flags);
ssize_t write(int fd, const void *buf, size_t count);

ssize_t read_size = recv(fd, &buf, sizeof(buf), 0);
ssize_t read_size = read(fd, &buf, sizeof(buf));
```

* 没有数据要发了，任意一端可以调用close或shutdown，发送FIN包，并进入FIN-WAIT-1状态
* close和shutdown区别：close调用后就不能读写了，并且使fd引用计数-1，当引用计数变为0的时候才发送FIN包；shutdown可以选择只关闭读或写，如果关闭写，当前缓冲区发送完后立即发送FIN包。如果只关闭写，还是可以继续读的

```c++
int close(int fd);
int shutdown(int sockfd, int how);

int res = close(fd);
int res = shutdown(fd, SHUT_WR); // SHUT_WR：只关闭写
```

### 三次握手

1. 客户端发送SYN包，此包SEQ为随机值A
2. 服务器收到SYN包后，发送SYN+ACK包，此包SEQ为随机值B，ACK为A+1
3. 客户端收到SYN+ACK包后，发送ACK包，此包SEQ为A+1，ACK为B+1

为什么是三次握手：

1. 服务器收到第一个包后知道客户端能发送到服务器
2. 客户端收到第二个包后知道客户端能发送到服务器、服务器能发送到客户端
3. 服务器收到第三个包后知道服务器能发送到客户端

### 四次挥手

1. 主动断开的一端发送FIN包
2. 被动断开的一端发送ACK包（小概率情况，自己也没有数据要发了，则发送FIN+ACK包）
3. 被动断开的一端应用层调用close了，没有数据要发了，此时发送FIN包
4. 主动断开的一端发送ACK包

为什么是四次挥手：

1. 被动断开的一端收到FIN包后，要回复ACK确认自己收到了，但是自己可能还有数据要发，此时不能发送FIN包
2. 被动断开的一端也没有数据要发后，再发送FIN包

为什么会有TIME-WAIT状态：

1. 不知道对端是否收到FIN包，要等待一段时间保证对端断开。如果对端没有重传则说明收到了
2. 防止重新用这个端口建立连接后，又收到了上一次连接的数据包。等待2MSL后可以保证数据包全部消逝

如何解决大量TIME-WAIT连接：

当本机有大量连接同时close后就会出现大量TIME-WAIT连接，会占用端口导致无法建立新连接。可以在Linux系统设置中启用`net.ipv4.tcp_tw_reuse`选项，允许客户端重用TIME-WAIT状态的连接。启用后内核会通过包的时间戳判断是不是旧连接的包

参考：

* [为什么 TCP 协议有 TIME_WAIT 状态](https://draveness.me/whys-the-design-tcp-time-wait/)

### 重传

重传时机：

* 超时重传：发送方使用一个估计的时间作为收到ACK的超时时间，这个时间被称为RTO（重传超时时间）。每次收到ACK时重置定时器。如果超时了还没收到ACK，则认为丢包了，需要重传

    * 正常情况下`RTO = 平滑化的RTT + 4 * RTT的标准差`
    * 丢包情况下`RTO * 2`（指数退避）

* 快速重传（基于重复累计确认的重传）：发送方如果收到3次对同一个包的ACK，则认为这个包后面的数据丢了，需要从最后一个未被确认的包开始重传

    * 因为ACK表示之前的SEQ都收到了，如果中间的包丢了，先收到了后面的包，此时只能ACK最后一个连续的包。发送方如果收到了多个相同的ACK，就知道中间有包丢了

### 确认

选择性确认（Selective ACK）：累计ACK的效率很低，即使只丢了一个包也要把后面所有的包重传，所以引入SACK，可以确认后面收到数据的范围，这样发送方只需要重传中间丢失的范围

D-SACK（Duplicate-SACK）：发送方可能会把乱序当做丢包，导致不必要的重传和拥塞避免，所以引入D-SACK。接收方发送D-SACK告诉发送端，没有丢包，只是乱序了，可以恢复到高发送速度

延迟确认：为了节省带宽，接收方不立即回复ACK，而是等待一段时间（Linux上是40ms），将多个ACK和要发送的数据合成一个包发送

Nagle算法：为了节省带宽，发送方会将多个小的数据合成一个包发送

* 大致的逻辑是：如果发送窗口和可发送的数据量都 >= MSS，则立即发送；否则判断是否有未确认的数据，如果有则先将发送数据放入缓冲区而不发送；如果没有未确认的数据则立即发送
* Nagle算法 + 延迟确认会导致更高的延迟，所以生产环境，特别是实时性要求高的游戏一般会关闭Nagle算法。关闭方法是对socket设置TCP_NODELAY选项

### 流量控制

流量控制用来避免发送得过快而接收方来不及完全接收，一般由接收方告诉发送方进行调控。TCP使用滑动窗口实现流量控制。接收方在每个包中告知自己可接收的大小，发送方在收到ACK包之前最多只能发这么多数据。收到ACK包后把窗口往后滑动，这样才可以发送新的数据

零窗口情况：当接收方宣布窗口为0时，发送方只能停止发送数据。此时发送方会启动一个保持定时器，以避免修改接收窗口的包丢失而死锁。当保持定时器超时时，发送方会发送一个零窗口探测包（Zero Window Probe），期待接收方回复一个带着新窗口大小的ACK。如果3次后还是零窗口，则发送RST把连接断了

### 拥塞控制

拥塞控制是发送方根据网络的承载情况控制发送速度。假如网络环境突然变差，会导致大量丢包和延迟，TCP对此只能重传，但是重传会导致网络负担更重。TCP是注重公平的协议，所以需要拥塞控制

发送方根据丢包情况估计网络拥塞情况。发送方自己维护一个拥塞窗口，这个窗口不用告知对端。发送窗口的大小是接收窗口和拥塞窗口的最小值

拥塞控制主要有以下几种算法：

* 慢启动：

    * 开始时，拥塞窗口设置为MSS的一个较小倍数（Linux下默认10倍）
    * 每次收到一个ACK（即过了一个RTT），拥塞窗口增加 `MSS * 这次确认包的个数`。这是指数增长
    * 当拥塞窗口到了慢启动阈值（ssthresh）后，则改用拥塞避免算法

* 拥塞避免：

    * 因为指数增长太快了，不能一直用指数增长，到达阈值后就改用线性增长
    * 在拥塞避免阶段，每次收到一个ACK，拥塞窗口只增加 `MSS * (1 + 1 / 拥塞窗口)`。每次增加一个包多一点点，且拥塞窗口越大，多的部分越小

* 拥塞发生/快速重传：

    * 上面讲过了，就是收到多次重复确认时，不用等待RTO，直接重传
    * 对拥塞窗口的处理两种算法有所不同：
        * Tahoe：当收到3个重复ACK时，将ssthresh设置为当前拥塞窗口的一半，拥塞窗口设置为1倍MSS，然后进入慢启动阶段
        * Reno：当收到3个重复ACK时，将ssthresh和拥塞窗口都设置为当前拥塞窗口的一半，然后进入快速恢复阶段
        * 但是对于RTO超时导致的重传，两种算法都是将拥塞窗口设置为1倍MSS，然后进入慢启动阶段。因为它们认为这种情况网络环境很糟糕，连ACK包都丢了

* 快速恢复：

    * 快速重传是Reno算法引入的新阶段
    * 这个阶段，发送方将会不停收到重复的ACK，直到重传的包被确认，才能回到拥塞避免阶段
    * 首先沉默半个窗口：因为之前发送的数据量是当前拥塞窗口的2倍，所以要等一段时间，使途中的数据量和当前拥塞窗口相等。这段时间既不会发送新的包，也不会在收到ACK包时增加拥塞窗口
    * 沉默半个窗口后，还要维持途中的数据量和当前拥塞窗口相等。收到1个重复的ACK包，说明除了第一个包丢失，后面有其他包被收到了，此时途中的包数量-1，可以发送一个新的包

## UDP

UDP是用户数据报协议，是无连接的、不可靠的、基于消息的传输层协议。其实就是在网络层的基础上加了端口，不像TCP那样有一套完整的机制

* 应用：DNS、NTP、DHCP、QUIC（HTTP3）、一些实时性要求高的网游
* 如何实现可靠UDP：需要应用层自己实现重传、排序等机制，常用的解决方案有KCP、QUIC、UDT

### 和TCP比较

* 基于消息：一个数据报就是一个完整的消息，但是大小受限于MTU；而TCP是基于流的，没有明确的边界，需要应用层自己加上消息大小，处理分包
* 不可靠：无法保证发送到目的地
* 无序：发给同一目标的消息不保证到达顺序
* 轻量级：没有连接、重传、排序、拥塞控制等机制
* 支持广播、多播：由于不需要和某个主机建立连接，UDP可以直接借助二、三层设备发送到多个主机

为什么UDP比TCP快：

1. 不需要建立连接就能直接发包
2. 没有流量控制、拥塞控制，想发多少包就发多少
3. 不需要确认包，头部大小比TCP小，能承载更大的带宽

## KCP

[参考](https://github.com/skywind3000/kcp)

KCP是一个自动重传请求（ARQ）协议，能以比TCP浪费10%-20%的带宽的代价，换取平均延迟降低30%-40%，且最大延迟降低2/3的传输效果。TCP讲究充分利用带宽，而KCP是尽量提高速度（减少单个数据包发送到另一端的时间）

为什么KCP快：

* RTO：TCP在超时重传时RTO \* 2；KCP在快速模式下是RTO \* 1.5
* 选择性重传：TCP会从丢的包开始全部重传；KCP只重传丢的包
* 快速重传：中间有包被跳过2次时，不用等RTO超时，直接重传
* 延迟ACK：TCP会延迟ACK，这样会算出较大的RTT；ACK可以选择是否延迟ACK
* 确认方式：ARQ模型响应有两种，UNA（此编号前所有包已收到，如TCP）和ACK（该编号包已收到），光用UNA将导致全部重传，光用ACK则丢失成本太高，以往协议都是二选其一，而KCP协议中，除去单独的ACK包外，所有包都有UNA信息
* 非退让流控：KCP可以阉割掉拥塞控制，使丢包时也不会减少发包速度，但是会牺牲公平性

# IO模型

* 同步/异步：结果的返回方式，如果是通过函数返回值就是同步；如果是通过回调或者事件就是异步
* 阻塞/非阻塞：阻塞就是在数据准备好之前线程进入休眠状态；非阻塞就是线程不进入休眠状态，而是立刻返回错误

## 常用的IO模型

### 同步阻塞IO

默认的模型，read的时候如果没有数据可读，则等待直到数据可读，读完后返回

* 常用的设计模式：主线程接受连接，来一个连接创建一个线程去处理
* 缺点：线程数量太多，会频繁切换上下文，占用CPU

### 同步非阻塞IO

通过fcntl设置fd为NONBLOCK，read的时候如果没有数据可读，直接返回错误码WOULDBLOCK，而不会等待

* 常用的设计模式：不停轮询读fd，直到读取成功。当然没成功的时候也可以去干其他事，比如处理定时器事件
* 缺点：轮询、系统调用浪费CPU

### IO多路复用

指一个线程处理多个fd的IO。使用select/poll/epoll阻塞等待多个fd，等到有数据可读再返回，这时可以直接去read了

* 常用的设计模式：Reactor模式，下面细说
* 缺点：这里的缺点是相对于异步IO的，1. read复制数据时还是阻塞的；2. 内核缓冲区和用户缓冲区是分开的，需要拷贝；3. 唤醒的时候数据还没到用户缓冲区，还需要自己调用read

### 异步IO

用户告诉内核需要read到哪个缓冲区，内核自己等待数据可读，然后内核直接拷贝数据到用户缓冲区，再通知用户读取完成了

* 常用的设计模式：Proactor模式
* 缺点：需要操作系统支持。Linux的aio_read是在用户态用线程模拟的，无法享受到真异步IO的好处，而且不支持buffered IO。Linux 5.1中引入的io_uring才是真异步接口。另外还有Windows的IOCP

## Reactor模式

[参考](https://www.zhihu.com/question/26943938)

Reactor模式是一种事件处理模式，为处理并发请求设计。Reactor模式分为3个对象：

* Reactor负责监听和分发事件，如果是连接事件则交给Acceptor，如果是读写事件则交给Handler
* Acceptor负责接受连接
* Handler负责读取和处理业务

### 单Reactor单线程

select、accept、read、write、处理业务都在一个线程

* 缺点：1. 单线程无法充分利用CPU；2. 处理业务时会阻塞网络IO
* 应用：Redis的单线程网络模型

### 单Reactor多线程

主线程select、accept、read、write，由线程池处理业务

* 缺点：读写事件耗时太多时可能影响接受连接

### 多Reactor多线程

也叫主从Reactor模式。主线程的主Reactor select、accept，其他线程的从Reactor select、read、write，另外还有线程池处理业务

* 优点：1. 分工明确；2. 每个线程IO压力变小，而且可以单独调整从Reactor数量
* 应用：Redis 6.0的多线程网络模型（但处理业务的线程还是只有一个）

## select/poll/epoll区别

这几个函数作用都是阻塞等待多个fd就绪，但是用法和性能上有区别

### select

```c++
// nfds：要监视的fd数，等于3个fds中最大的fd + 1，这个参数只是为了不用遍历完整的1024个fd
// fds：文件集合，数据结构是bitmap，输入要监视的fd，输出就绪的fd
int select(int nfds, fd_set *restrict readfds,
           fd_set *restrict writefds, fd_set *restrict exceptfds,
           struct timeval *restrict timeout);
// 用来操作fds的宏
void FD_CLR(int fd, fd_set *set);
int  FD_ISSET(int fd, fd_set *set);
void FD_SET(int fd, fd_set *set);
void FD_ZERO(fd_set *set);
```

用法：

1. 每次循环，先FD_ZERO清空fds，再用FD_SET设置要监视的fd
2. 调用select，返回后fds中为1的位表示这个fd就绪
3. 遍历fds找出哪些fd就绪

缺点：

* 最大的缺点是最多只能监视1024个fd
* 内核实现是轮询fd检查是否就绪，浪费CPU，并且fd增加时性能变差（O(n)）
* 内核需要遍历fds，找出要监视的fd；用户也需要遍历fds，找出就绪的fd
* 调用时fds从用户缓冲区拷贝到内核缓冲区；返回时从内核缓冲区拷贝到用户缓冲区
* 只支持水平触发，即如果返回后不read，下次返回还是会报告read就绪事件

### poll

```c++
int poll(struct pollfd *fds, nfds_t nfds, int timeout);
struct pollfd {
    int   fd;         /* 要监视的fd */
    short events;     /* 要监视的事件 */
    short revents;    /* 返回就绪的事件 */
};
```

用法：

1. 准备一个fds数组，存放要监视的fd和事件
2. 调用poll，返回后revents中1的位表示这个事件就绪
3. 遍历fds，找出就绪的fd和事件

缺点：

跟select相比，除了它底层用链表存放fds，没有fd数量限制，其他缺点都有

### epoll

```c++
int epoll_create(int size);
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
int epoll_wait(int epfd, struct epoll_event *events,
               int maxevents, int timeout);

struct epoll_event {
    uint32_t     events;    /* Epoll events */
    epoll_data_t data;      /* User data variable */
};

// 从Linux 2.6.8开始，size参数被忽略，但是必须 > 0
int epfd = epoll_create(1);

// 添加要监视的fd和事件。fd只在这里向内核拷贝一次，后续epoll_wait不需要再向内核拷贝
epoll_event event{};
event.events = EPOLLIN | EPOLLET; // 监视读就绪事件，边缘触发
event.data.fd = fd; // data是返回给用户的上下文，随便设置
epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &event);

// 等待就绪事件
epoll_event ret_events[100];
int ret_num = epoll_wait(epfd, ret_events, 100, -1);
```

用法：

1. 创建epoll对象（返回一个fd）
2. 调用epoll_ctl添加要监视的fd和事件
3. 调用epoll_wait，返回后内核把事件拷贝到用户缓冲区
4. 遍历返回的事件并处理

优点：

* 内核用回调实现，不需要轮询。fd增加时性能几乎不变（O(1)）
    * 首次添加fd时，向驱动程序注册回调。回调把fd添加到就绪列表中，并唤醒正在epoll_wait的进程
    * epoll_wait只关心这个就绪列表里的fd，而不去轮询所有fd
* 内核中fds的数据结构是用红黑树，读写的时间复杂度是O(logn)。就绪列表的数据结构是链表，读写的时间复杂度是O(1)
* 只有添加fd的时候向内核拷贝一次，不用每次等待都拷贝
* 内核只拷贝就绪的事件到用户态，不用拷贝所有事件。用户也只需遍历就绪的事件，不用遍历所有的事件
    * 有人说这里事件列表会用mmap，内核和用户态共享，不需要拷贝，实际上没用
* 支持边缘触发，即只有事件第一次发生时报告

缺点：

* 只有Linux能用，而select是POSIX标准
